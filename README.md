# CoT-Compression
# 👀 Introduction
近年来，大语言模型（LLMs）通过引入思维链（Chain-of-Thought, CoT）推理显著提升了复杂任务的解决能力，例如数学推理和逻辑推理。然而，CoT方法通常依赖于生成冗长的中间推理步骤，导致计算开销激增、推理延迟增加，并阻碍其在资源受限场景（如边缘设备或实时系统）中的应用。为解决这一矛盾，研究者提出了CoT压缩技术，旨在通过优化推理路径、提炼关键步骤或动态调整计算复杂度，在保持模型性能的前提下大幅降低推理成本。因此，我们维护一个最新的 GitHub 仓库，以跟踪这一快速发展领域的最新进展。
# 📒 Table of Contents
- TokenSkip [[GitHub]](https://github.com/hemingkx/TokenSkip)
- Rho-1 [[GitHub]](https://github.com/microsoft/rho)
- SelectiveDPO(Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples) [[GitHub]](https://github.com/glorgao/SelectiveDPO)
- CoT-Valve: Length-Compressible Chain-of-Thought Tuning [[pdf]](https://arxiv.org/abs/2502.09601)
- Chain of Draft: Thinking Faster by Writing Less [[pdf]](https://arxiv.org/abs/2502.18600)
- LightThinker: Thinking Step-by-Step Compression [[pdf]](https://arxiv.org/abs/2502.15589)
- Understanding Chain-of-Thought in LLMs through Information Theory [[pdf]](https://arxiv.org/abs/2411.11984)
- When More is Less: Understanding Chain-of-Thought Length in LLMs [[pdf]](https://arxiv.org/abs/2502.07266)
- Dynamic Chain-of-Thought: Towards Adaptive Deep Reasoning [[pdf]](https://arxiv.org/abs/2502.10428)
- Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models [[pdf]](https://arxiv.org/abs/2502.13260)
- Distilling System 2 into System 1 [[pdf]](https://arxiv.org/abs/2502.10428)
- Efficiently Serving LLM Reasoning Programs with Certaindex [[pdf]](https://arxiv.org/abs/2412.20993)
- Fast Best-of-N Decoding via Speculative Rejection [[pdf]](https://arxiv.org/abs/2410.20290)
