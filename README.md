# -CoT-Compression
# ðŸ‘€ Introduction
# ðŸ“’ Table of Contents
- TokenSkip [[GitHub]](https://github.com/hemingkx/TokenSkip)
- CoT-Valve: Length-Compressible Chain-of-Thought Tuning [[pdf]](https://arxiv.org/abs/2502.09601)
- Chain of Draft: Thinking Faster by Writing Less [[pdf]](https://arxiv.org/abs/2502.18600)
- LightThinker: Thinking Step-by-Step Compression [[pdf]](https://arxiv.org/abs/2502.15589)
- Understanding Chain-of-Thought in LLMs through Information Theory [[pdf]](https://arxiv.org/abs/2411.11984)
- When More is Less: Understanding Chain-of-Thought Length in LLMs [[pdf]](https://arxiv.org/abs/2502.07266)
- Dynamic Chain-of-Thought: Towards Adaptive Deep Reasoning [[pdf]](https://arxiv.org/abs/2502.10428)
- Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models [[pdf]](https://arxiv.org/abs/2502.13260)
- Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples [[pdf]](https://arxiv.org/abs/2502.09650)
- Rho-1: Not All Tokens Are What You Need [[pdf]](https://arxiv.org/abs/2404.07965)
- Distilling System 2 into System 1 [[pdf]](https://arxiv.org/abs/2502.10428)
- Efficiently Serving LLM Reasoning Programs with Certaindex [[pdf]](https://arxiv.org/abs/2412.20993)
- Fast Best-of-N Decoding via Speculative Rejection [[pdf]](https://arxiv.org/abs/2410.20290)
